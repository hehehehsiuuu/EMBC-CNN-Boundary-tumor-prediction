{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline includes:Multi-seed (≥3, ideally 5)\n",
        "\n",
        "You’re running 5 seeds: SEEDS = [42, 1337, 2026, 7, 99]\n",
        "\n",
        "You also export:\n",
        "\n",
        "per-seed metrics\n",
        "\n",
        "embc_paper_master_summary_all_seeds.csv\n",
        "\n",
        "across-seed aggregated embc_summary_across_seeds.csv (mean/std across seeds)\n",
        "\n",
        "ASSD + HD95\n",
        "\n",
        "Added as boundary-distance metrics: ASSD_px, HD95_px\n",
        "\n",
        "Computed from boundary pixel sets using distance transforms (standard approach)\n",
        "\n",
        "Weighted boundary loss\n",
        "\n",
        "Computes a pos_weight from training boundary targets (estimate_pos_weight)\n",
        "\n",
        "Uses weighted cross entropy (logits) + Dice via boundary_loss_factory(pos_w)\n",
        "\n",
        "This is exactly what you need to prevent the “predict-everything / predict-nothing” instability in sparse boundaries.\n",
        "\n",
        "Threshold tuned to tolerant-F1\n",
        "\n",
        "find_best_global_threshold() now tunes threshold based on mean tolerant F1 across [1,2,3,5] px, not Dice.\n",
        "\n",
        "Same for UNet threshold tuning."
      ],
      "metadata": {
        "id": "rO8TUpHjuCzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!pip install -q nibabel scikit-image pandas scipy\n",
        "\n",
        "import os, zipfile, glob, random, math, shutil, json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from scipy.ndimage import distance_transform_edt, binary_erosion\n",
        "\n",
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "\n",
        "# multi-seed\n",
        "SEEDS = [42, 1337, 2026, 7, 99]\n",
        "\n",
        "PRETRAIN_SOBEL_EPOCHS = 2\n",
        "FINETUNE_BOUNDARY_EPOCHS = 8\n",
        "FINETUNE_UNET_EPOCHS = 8\n",
        "\n",
        "RUN_SMALL_CNN_BASELINE   = True\n",
        "RUN_SOBELFRONT_CNN       = True\n",
        "RUN_TINY_UNET_SEG_BASE   = True  # trains on mask; evaluated as boundary\n",
        "\n",
        "MAX_SLICES_PER_VOLUME = 6\n",
        "\n",
        "TRAIN_FRAC = 0.70\n",
        "VAL_FRAC   = 0.15\n",
        "# remainder = TEST_LABELED\n",
        "\n",
        "# Eval + figs\n",
        "EVAL_LIMIT = None    # e.g., 200 for fast debug\n",
        "VIS_K = 3\n",
        "THRESHOLDS = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9]\n",
        "TOL_PX_LIST = [1,2,3,5]\n",
        "\n",
        "# Boundary target / imbalance handling\n",
        "SKIP_EMPTY_MASK_SLICES = True\n",
        "BOUNDARY_THICKNESS = 2\n",
        "\n",
        "# File formats\n",
        "VALID_2D_EXTS = {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}\n",
        "MASK_TOKENS = [\"_mask\", \"mask\", \"seg\", \"label\", \"_gt\", \"groundtruth\"]\n",
        "\n",
        "# Drive / BraTS\n",
        "USE_GOOGLE_DRIVE = True\n",
        "BRATS_ZIP_PATH = \"/content/drive/MyDrive/Brats Dataset.zip\"\n",
        "BRATS_EXTRACT_DIR = \"/content/brats_extracted\"\n",
        "BRATS_TRAIN_DIR = None\n",
        "BRATS_VAL_DIR   = None\n",
        "EXTRA_DATASET_DIRS = {\n",
        "  # \"BUSI\": \"/content/drive/MyDrive/BUSI\",\n",
        "}\n",
        "\n",
        "# Reproducibility helpers\n",
        "def set_all_seeds(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.keras.utils.set_random_seed(seed)\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('image', cmap='gray')\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "print(\"IMG_SIZE:\", IMG_SIZE)\n",
        "# detection + pairing\n",
        "def is_nii(path):\n",
        "    low = path.lower()\n",
        "    return low.endswith(\".nii\") or low.endswith(\".nii.gz\")\n",
        "\n",
        "def ext(path):\n",
        "    return os.path.splitext(path)[1].lower()\n",
        "\n",
        "def is_2d_image(path):\n",
        "    return ext(path) in VALID_2D_EXTS\n",
        "\n",
        "def is_mask_file(path):\n",
        "    name = os.path.basename(path).lower()\n",
        "    return any(tok in name for tok in MASK_TOKENS)\n",
        "\n",
        "def strip_nii_ext(name):\n",
        "    name = name.lower()\n",
        "    if name.endswith(\".nii.gz\"):\n",
        "        return name[:-7]\n",
        "    if name.endswith(\".nii\"):\n",
        "        return name[:-4]\n",
        "    return os.path.splitext(name)[0].lower()\n",
        "\n",
        "def norm_key_2d(path):\n",
        "    base = os.path.splitext(os.path.basename(path).lower())[0]\n",
        "    for tok in MASK_TOKENS:\n",
        "        base = base.replace(tok, \"\")\n",
        "    base = base.replace(\"-\", \"_\").replace(\" \", \"_\")\n",
        "    base = base.replace(\"__\", \"_\").strip(\"_\")\n",
        "    return base\n",
        "\n",
        "MOD_SUFFIX = {\"flair\",\"t1\",\"t1ce\",\"t2\",\"seg\",\"mask\",\"label\",\"gt\",\"groundtruth\"}\n",
        "def norm_key_nii(path):\n",
        "    base = strip_nii_ext(os.path.basename(path))\n",
        "    parts = base.split(\"_\")\n",
        "    if len(parts) > 1 and parts[-1] in MOD_SUFFIX:\n",
        "        parts = parts[:-1]\n",
        "    return \"_\".join(parts)\n",
        "\n",
        "def group_id(desc):\n",
        "    # patient-level grouping for BraTS (nii) and best-effort grouping for 2D\n",
        "    if desc[0] == \"nii\":\n",
        "        return norm_key_nii(desc[1])\n",
        "    if desc[0] == \"2d\":\n",
        "        # if BUSI folders store per-patient, you can use parent folder\n",
        "        # return os.path.basename(os.path.dirname(desc[1]))\n",
        "        return norm_key_2d(desc[1])\n",
        "    return \"unknown\"\n",
        "\n",
        "def safe_read_2d_as_tensor(path):\n",
        "    try:\n",
        "        raw = tf.io.read_file(path)\n",
        "        img = tf.io.decode_image(raw, channels=1, expand_animations=False)\n",
        "        img = tf.image.resize(img, IMG_SIZE)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "        if img.shape[-1] != 1:\n",
        "            return None\n",
        "        return img\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def resize_mask_nearest(mask01):\n",
        "    return tf.image.resize(mask01, IMG_SIZE, method=\"nearest\")\n",
        "\n",
        "def scan_files(root):\n",
        "    all_files = [f for f in glob.glob(os.path.join(root, \"**/*\"), recursive=True) if os.path.isfile(f)]\n",
        "    img2d = [f for f in all_files if is_2d_image(f)]\n",
        "    nii   = [f for f in all_files if is_nii(f)]\n",
        "    return all_files, img2d, nii\n",
        "\n",
        "def build_index_for_dataset(root):\n",
        "    \"\"\"\n",
        "    samples:\n",
        "      (\"2d\", img_path, mask_path_or_None)\n",
        "      (\"nii\", img_vol_path, seg_vol_path_or_None, z_index)\n",
        "    \"\"\"\n",
        "    _, img2d_all, nii_all = scan_files(root)\n",
        "    samples, has_masks = [], False\n",
        "\n",
        "    # 2D pairing\n",
        "    if len(img2d_all) > 0:\n",
        "        imgs  = [p for p in img2d_all if not is_mask_file(p)]\n",
        "        masks = [p for p in img2d_all if is_mask_file(p)]\n",
        "\n",
        "        mask_map = {}\n",
        "        for m in masks:\n",
        "            mask_map.setdefault(norm_key_2d(m), []).append(m)\n",
        "\n",
        "        for im in imgs:\n",
        "            key = norm_key_2d(im)\n",
        "            mlist = mask_map.get(key, [])\n",
        "            mpath = mlist[0] if len(mlist) > 0 else None\n",
        "            if mpath is not None:\n",
        "                has_masks = True\n",
        "            samples.append((\"2d\", im, mpath))\n",
        "\n",
        "    # NIfTI pairing (BraTS-like)\n",
        "    if len(nii_all) > 0:\n",
        "        nii_imgs  = [p for p in nii_all if not is_mask_file(p)]\n",
        "        nii_masks = [p for p in nii_all if is_mask_file(p)]\n",
        "        mask_map = {norm_key_nii(m): m for m in nii_masks}\n",
        "\n",
        "        flair = [f for f in nii_imgs if \"flair\" in os.path.basename(f).lower()]\n",
        "        use_inputs = flair if len(flair) > 0 else nii_imgs\n",
        "\n",
        "        for vf in use_inputs:\n",
        "            seg = mask_map.get(norm_key_nii(vf), None)\n",
        "            if seg is not None:\n",
        "                has_masks = True\n",
        "\n",
        "            try:\n",
        "                img = nib.load(vf)\n",
        "                shp = img.shape\n",
        "                if len(shp) < 3:\n",
        "                    continue\n",
        "                zdim = shp[2]\n",
        "                zs = np.linspace(zdim*0.3, zdim*0.7, MAX_SLICES_PER_VOLUME).astype(int)\n",
        "                zs = np.unique(np.clip(zs, 0, zdim-1))\n",
        "                for z in zs:\n",
        "                    samples.append((\"nii\", vf, seg, int(z)))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    return samples, has_masks\n",
        "\n",
        "# ----------------------------\n",
        "# TARGETS\n",
        "# ----------------------------\n",
        "def sobel_target(img01):\n",
        "    img01 = tf.convert_to_tensor(img01, dtype=tf.float32)\n",
        "    sob = tf.image.sobel_edges(img01[None])[0]  # (H,W,1,2)\n",
        "    gx, gy = sob[...,0], sob[...,1]\n",
        "    mag = tf.sqrt(gx**2 + gy**2)\n",
        "    mag = mag / (tf.reduce_max(mag) + 1e-8)\n",
        "    return mag\n",
        "\n",
        "def mask_boundary_target(mask01):\n",
        "    \"\"\"\n",
        "    boundary = dilation(mask) - erosion(mask), then thicken.\n",
        "    \"\"\"\n",
        "    m = tf.cast(mask01 > 0.5, tf.float32)\n",
        "    dil = tf.nn.max_pool2d(m[None], ksize=3, strides=1, padding=\"SAME\")[0]\n",
        "    ero = 1.0 - tf.nn.max_pool2d((1.0 - m)[None], ksize=3, strides=1, padding=\"SAME\")[0]\n",
        "    b = tf.clip_by_value(dil - ero, 0.0, 1.0)\n",
        "\n",
        "    if BOUNDARY_THICKNESS > 1:\n",
        "        for _ in range(BOUNDARY_THICKNESS - 1):\n",
        "            b = tf.nn.max_pool2d(b[None], ksize=3, strides=1, padding=\"SAME\")[0]\n",
        "        b = tf.clip_by_value(b, 0.0, 1.0)\n",
        "\n",
        "    return b\n",
        "\n",
        "# FIXED SOBEL baseline\n",
        "def fixed_sobel_conv(img01):\n",
        "    img01 = tf.convert_to_tensor(img01, dtype=tf.float32)\n",
        "    kx = tf.constant([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=tf.float32)\n",
        "    ky = tf.constant([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=tf.float32)\n",
        "    kx = tf.reshape(kx, (3,3,1,1))\n",
        "    ky = tf.reshape(ky, (3,3,1,1))\n",
        "    gx = tf.nn.conv2d(img01[None], kx, strides=1, padding=\"SAME\")[0]\n",
        "    gy = tf.nn.conv2d(img01[None], ky, strides=1, padding=\"SAME\")[0]\n",
        "    mag = tf.sqrt(gx**2 + gy**2)\n",
        "    mag = mag / (tf.reduce_max(mag) + 1e-8)\n",
        "    return mag\n",
        "\n",
        "def load_sample(desc, target_mode=\"auto\"):\n",
        "    \"\"\"\n",
        "    target_mode:\n",
        "      - \"sobel\":    target = sobel(input)\n",
        "      - \"boundary\": target = boundary(mask) (requires mask)\n",
        "      - \"mask\":     target = mask (requires mask)\n",
        "      - \"auto\":     boundary if mask exists else sobel\n",
        "    \"\"\"\n",
        "    kind = desc[0]\n",
        "\n",
        "    if kind == \"2d\":\n",
        "        img_path, mask_path = desc[1], desc[2]\n",
        "        img = safe_read_2d_as_tensor(img_path)\n",
        "        if img is None:\n",
        "            return None\n",
        "        img01 = img\n",
        "\n",
        "        use_mask = (mask_path is not None)\n",
        "        mode = target_mode if target_mode != \"auto\" else (\"boundary\" if use_mask else \"sobel\")\n",
        "\n",
        "        if mode == \"sobel\":\n",
        "            return img01, sobel_target(img01)\n",
        "\n",
        "        if mode in [\"boundary\",\"mask\"]:\n",
        "            if not use_mask:\n",
        "                return None\n",
        "            m = safe_read_2d_as_tensor(mask_path)\n",
        "            if m is None:\n",
        "                return None\n",
        "            m = resize_mask_nearest(m)\n",
        "            m = tf.cast(m > 0.5, tf.float32)\n",
        "            if SKIP_EMPTY_MASK_SLICES and tf.reduce_max(m).numpy() == 0.0:\n",
        "                return None\n",
        "            if mode == \"mask\":\n",
        "                return img01, m\n",
        "            return img01, mask_boundary_target(m)\n",
        "\n",
        "        return None\n",
        "\n",
        "    if kind == \"nii\":\n",
        "        img_vol, seg_vol, z = desc[1], desc[2], desc[3]\n",
        "        try:\n",
        "            img = nib.load(img_vol)\n",
        "            data = img.dataobj\n",
        "            shp = img.shape\n",
        "            sl = np.asanyarray(data[:, :, z]).astype(np.float32) if len(shp) == 3 else np.asanyarray(data[:, :, z, 0]).astype(np.float32)\n",
        "            mn, mx = float(np.min(sl)), float(np.max(sl))\n",
        "            sl = (sl - mn) / (mx - mn + 1e-8)\n",
        "\n",
        "            img01 = tf.convert_to_tensor(sl[..., None], dtype=tf.float32)\n",
        "            img01 = tf.image.resize(img01, IMG_SIZE)\n",
        "\n",
        "            use_mask = (seg_vol is not None)\n",
        "            mode = target_mode if target_mode != \"auto\" else (\"boundary\" if use_mask else \"sobel\")\n",
        "\n",
        "            if mode == \"sobel\":\n",
        "                return img01, sobel_target(img01)\n",
        "\n",
        "            if mode in [\"boundary\",\"mask\"]:\n",
        "                if not use_mask:\n",
        "                    return None\n",
        "                seg = nib.load(seg_vol)\n",
        "                segd = seg.dataobj\n",
        "                sshape = seg.shape\n",
        "                msl = np.asanyarray(segd[:, :, z]).astype(np.float32) if len(sshape) == 3 else np.asanyarray(segd[:, :, z, 0]).astype(np.float32)\n",
        "\n",
        "                msl = (msl > 0).astype(np.float32)  # BraTS tumor mask\n",
        "                if SKIP_EMPTY_MASK_SLICES and float(np.max(msl)) == 0.0:\n",
        "                    return None\n",
        "\n",
        "                m01 = tf.convert_to_tensor(msl[..., None], dtype=tf.float32)\n",
        "                m01 = resize_mask_nearest(m01)\n",
        "                if mode == \"mask\":\n",
        "                    return img01, m01\n",
        "                return img01, mask_boundary_target(m01)\n",
        "\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "def load_input_only(desc):\n",
        "    kind = desc[0]\n",
        "    if kind == \"2d\":\n",
        "        return safe_read_2d_as_tensor(desc[1])\n",
        "    if kind == \"nii\":\n",
        "        img_vol, z = desc[1], desc[3]\n",
        "        try:\n",
        "            img = nib.load(img_vol)\n",
        "            data = img.dataobj\n",
        "            shp = img.shape\n",
        "            sl = np.asanyarray(data[:, :, z]).astype(np.float32) if len(shp) == 3 else np.asanyarray(data[:, :, z, 0]).astype(np.float32)\n",
        "            mn, mx = float(np.min(sl)), float(np.max(sl))\n",
        "            sl = (sl - mn) / (mx - mn + 1e-8)\n",
        "            img01 = tf.convert_to_tensor(sl[..., None], dtype=tf.float32)\n",
        "            img01 = tf.image.resize(img01, IMG_SIZE)\n",
        "            return img01\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def make_tf_dataset(samples, target_mode=\"auto\", shuffle=True, repeat=False, seed=42):\n",
        "    def gen():\n",
        "        for d in samples:\n",
        "            out = load_sample(d, target_mode=target_mode)\n",
        "            if out is None:\n",
        "                continue\n",
        "            yield out\n",
        "\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(IMG_SIZE[0], IMG_SIZE[1], 1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(IMG_SIZE[0], IMG_SIZE[1], 1), dtype=tf.float32),\n",
        "        )\n",
        "    )\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(256, seed=seed, reshuffle_each_iteration=True)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "    return ds\n",
        "\n",
        "def estimate_steps(samples):\n",
        "    return max(1, math.ceil(len(samples) / BATCH_SIZE))\n",
        "\n",
        "def split_by_group(samples, train_frac=0.70, val_frac=0.15, seed=42):\n",
        "    rng = random.Random(seed)\n",
        "    groups = {}\n",
        "    for s in samples:\n",
        "        gid = group_id(s)\n",
        "        groups.setdefault(gid, []).append(s)\n",
        "    gids = list(groups.keys())\n",
        "    rng.shuffle(gids)\n",
        "\n",
        "    n = len(gids)\n",
        "    n_train = int(train_frac * n)\n",
        "    n_val   = int(val_frac * n)\n",
        "\n",
        "    g_train = set(gids[:n_train])\n",
        "    g_val   = set(gids[n_train:n_train+n_val])\n",
        "    g_test  = set(gids[n_train+n_val:])\n",
        "\n",
        "    train = [s for gid in g_train for s in groups[gid]]\n",
        "    val   = [s for gid in g_val   for s in groups[gid]]\n",
        "    test  = [s for gid in g_test  for s in groups[gid]]\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "# MODELS\n",
        "def sobel_init_weights():\n",
        "    gx = np.array([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=np.float32)\n",
        "    gy = np.array([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=np.float32)\n",
        "    w = np.zeros((3,3,1,2), dtype=np.float32)\n",
        "    w[:,:,0,0] = gx\n",
        "    w[:,:,0,1] = gy\n",
        "    return w\n",
        "\n",
        "class LearnableSobelEdge(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Minimal learnable(boundary) baseline:\n",
        "      Conv(2,3x3) Sobel-init -> magnitude -> (optional per-image norm)\n",
        "      -> alpha/beta -> sigmoid (during boundary training)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            filters=2, kernel_size=3, padding=\"same\", use_bias=False,\n",
        "            kernel_initializer=tf.keras.initializers.Constant(sobel_init_weights())\n",
        "        )\n",
        "        self.alpha = self.add_weight(name=\"alpha\", shape=(), initializer=\"ones\", trainable=True, dtype=tf.float32)\n",
        "        self.beta  = self.add_weight(name=\"beta\",  shape=(), initializer=\"zeros\", trainable=True, dtype=tf.float32)\n",
        "        self.use_per_image_norm = True\n",
        "        self.use_sigmoid = False\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        g = self.conv(x)\n",
        "        gx = g[:,:,:,0:1]\n",
        "        gy = g[:,:,:,1:2]\n",
        "        mag = tf.sqrt(gx*gx + gy*gy + 1e-8)\n",
        "\n",
        "        if self.use_per_image_norm:\n",
        "            mx = tf.reduce_max(tf.abs(mag), axis=[1,2,3], keepdims=True)\n",
        "            mag = mag / (mx + 1e-8)\n",
        "\n",
        "        mag = self.alpha * mag + self.beta\n",
        "\n",
        "        if self.use_sigmoid:\n",
        "            return tf.sigmoid(mag)\n",
        "        return mag\n",
        "\n",
        "def build_learnable_model(loss_obj):\n",
        "    m = LearnableSobelEdge()\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=loss_obj)\n",
        "    return m\n",
        "\n",
        "def build_small_cnn_boundary(loss_obj):\n",
        "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))\n",
        "    x = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(inp)\n",
        "    x = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    out = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(x)\n",
        "    m = tf.keras.Model(inp, out, name=\"SmallCNN_Boundary\")\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=loss_obj)\n",
        "    return m\n",
        "\n",
        "def build_sobelfront_cnn(loss_obj):\n",
        "    \"\"\"\n",
        "    Sobel-init front-end + small CNN head (Keras 3 safe).\n",
        "    \"\"\"\n",
        "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))\n",
        "    sob = tf.keras.layers.Conv2D(\n",
        "        filters=2, kernel_size=3, padding=\"same\", use_bias=False,\n",
        "        kernel_initializer=tf.keras.initializers.Constant(sobel_init_weights()),\n",
        "        name=\"SobelInitConv\"\n",
        "    )(inp)\n",
        "\n",
        "    gx = tf.keras.layers.Lambda(lambda t: t[..., 0:1], name=\"gx\")(sob)\n",
        "    gy = tf.keras.layers.Lambda(lambda t: t[..., 1:2], name=\"gy\")(sob)\n",
        "    mag = tf.keras.layers.Lambda(lambda ab: tf.sqrt(ab[0]*ab[0] + ab[1]*ab[1] + 1e-8), name=\"mag\")([gx, gy])\n",
        "\n",
        "    x = tf.keras.layers.Concatenate(name=\"stack_feats\")([inp, mag, gx, gy])\n",
        "    x = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Conv2D(16, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    out = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(x)\n",
        "\n",
        "    m = tf.keras.Model(inp, out, name=\"SobelFrontCNN\")\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=loss_obj)\n",
        "    return m\n",
        "\n",
        "def conv_block(x, f):\n",
        "    x = tf.keras.layers.Conv2D(f, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Conv2D(f, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "def build_tiny_unet_seg(loss_obj):\n",
        "    \"\"\"\n",
        "    Tiny U-Net predicting mask (sigmoid). We'll convert its output to boundary for evaluation.\n",
        "    \"\"\"\n",
        "    inp = tf.keras.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 1))\n",
        "    c1 = conv_block(inp, 16)\n",
        "    p1 = tf.keras.layers.MaxPool2D()(c1)\n",
        "\n",
        "    c2 = conv_block(p1, 32)\n",
        "    p2 = tf.keras.layers.MaxPool2D()(c2)\n",
        "\n",
        "    b  = conv_block(p2, 64)\n",
        "\n",
        "    u2 = tf.keras.layers.UpSampling2D()(b)\n",
        "    u2 = tf.keras.layers.Concatenate()([u2, c2])\n",
        "    c3 = conv_block(u2, 32)\n",
        "\n",
        "    u1 = tf.keras.layers.UpSampling2D()(c3)\n",
        "    u1 = tf.keras.layers.Concatenate()([u1, c1])\n",
        "    c4 = conv_block(u1, 16)\n",
        "\n",
        "    out = tf.keras.layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(c4)\n",
        "    m = tf.keras.Model(inp, out, name=\"TinyUNet_Seg\")\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=loss_obj)\n",
        "    return m\n",
        "\n",
        "\n",
        "# LOSSES\n",
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "def soft_dice_coef(y_true, y_pred, eps=1e-6):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "    y_true_f = tf.reshape(y_true, [tf.shape(y_true)[0], -1])\n",
        "    y_pred_f = tf.reshape(y_pred, [tf.shape(y_pred)[0], -1])\n",
        "    inter = tf.reduce_sum(y_true_f * y_pred_f, axis=1)\n",
        "    denom = tf.reduce_sum(y_true_f + y_pred_f, axis=1)\n",
        "    dice = (2.0 * inter + eps) / (denom + eps)\n",
        "    return tf.reduce_mean(dice)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - soft_dice_coef(y_true, y_pred)\n",
        "\n",
        "def seg_loss(y_true, y_pred):\n",
        "    return bce(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "\n",
        "def sobel_mse_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Weighted BCE (imbalance)\n",
        "def estimate_pos_weight(samples, limit=300):\n",
        "    pos, neg, count = 0, 0, 0\n",
        "    for d in samples:\n",
        "        out = load_sample(d, target_mode=\"boundary\")\n",
        "        if out is None:\n",
        "            continue\n",
        "        _, gt = out\n",
        "        g = gt.numpy().astype(np.uint8)\n",
        "        pos += int(g.sum())\n",
        "        neg += int(g.size - g.sum())\n",
        "        count += 1\n",
        "        if limit and count >= limit:\n",
        "            break\n",
        "    pw = float(neg / (pos + 1e-9))\n",
        "    pw = max(1.0, min(50.0, pw))  # clamp to avoid instability\n",
        "    return pw\n",
        "\n",
        "def weighted_bce_with_logits(pos_weight):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), 1e-6, 1.0 - 1e-6)\n",
        "        logits = tf.math.log(y_pred / (1.0 - y_pred))\n",
        "        return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
        "            labels=y_true, logits=logits, pos_weight=pos_weight\n",
        "        ))\n",
        "    return loss\n",
        "\n",
        "def boundary_loss_factory(pos_weight):\n",
        "    wbce = weighted_bce_with_logits(pos_weight)\n",
        "    def loss(y_true, y_pred):\n",
        "        return wbce(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# METRICS\n",
        "def bin_metrics(y_true01, y_pred01, thr=0.5, eps=1e-9):\n",
        "    yt = (y_true01 >= 0.5).astype(np.uint8)\n",
        "    yp = (y_pred01 >= thr).astype(np.uint8)\n",
        "    tp = int(np.sum((yt==1) & (yp==1)))\n",
        "    fp = int(np.sum((yt==0) & (yp==1)))\n",
        "    fn = int(np.sum((yt==1) & (yp==0)))\n",
        "    dice = (2*tp) / (2*tp + fp + fn + eps)\n",
        "    prec = tp / (tp + fp + eps)\n",
        "    rec  = tp / (tp + fn + eps)\n",
        "    return dice, prec, rec\n",
        "\n",
        "def tolerant_f1(gt_b, pr_b, tol_px, eps=1e-9):\n",
        "    \"\"\"\n",
        "    Tolerant boundary F1:\n",
        "      pred pixel is correct if within tol_px of any GT pixel; and vice versa.\n",
        "    \"\"\"\n",
        "    gt = (gt_b > 0).astype(bool)\n",
        "    pr = (pr_b > 0).astype(bool)\n",
        "\n",
        "    if gt.sum() == 0 and pr.sum() == 0:\n",
        "        return 1.0\n",
        "    if gt.sum() == 0 and pr.sum() > 0:\n",
        "        return 0.0\n",
        "    if gt.sum() > 0 and pr.sum() == 0:\n",
        "        return 0.0\n",
        "\n",
        "    dist_to_gt = distance_transform_edt(~gt)\n",
        "    dist_to_pr = distance_transform_edt(~pr)\n",
        "\n",
        "    tp_pred = np.sum(pr & (dist_to_gt <= tol_px))\n",
        "    tp_gt   = np.sum(gt & (dist_to_pr <= tol_px))\n",
        "\n",
        "    prec = tp_pred / (pr.sum() + eps)\n",
        "    rec  = tp_gt   / (gt.sum() + eps)\n",
        "    f1 = (2*prec*rec) / (prec+rec+eps)\n",
        "    return float(f1)\n",
        "\n",
        "# ASSD + HD95 boundary metrics (pixels)\n",
        "def boundary_map(bin_mask):\n",
        "    m = bin_mask.astype(bool)\n",
        "    if m.sum() == 0:\n",
        "        return np.zeros_like(m, dtype=bool)\n",
        "    er = binary_erosion(m)\n",
        "    return m ^ er\n",
        "\n",
        "def assd_hd95(gt_bin, pr_bin):\n",
        "    \"\"\"\n",
        "    ASSD and HD95 computed on boundary pixels (pixel units).\n",
        "    \"\"\"\n",
        "    gt_b = boundary_map(gt_bin)\n",
        "    pr_b = boundary_map(pr_bin)\n",
        "\n",
        "    if gt_b.sum() == 0 and pr_b.sum() == 0:\n",
        "        return 0.0, 0.0\n",
        "    if gt_b.sum() == 0 or pr_b.sum() == 0:\n",
        "        return float(\"inf\"), float(\"inf\")\n",
        "\n",
        "    dt_gt = distance_transform_edt(~gt_b)\n",
        "    dt_pr = distance_transform_edt(~pr_b)\n",
        "\n",
        "    d_pr_to_gt = dt_gt[pr_b]\n",
        "    d_gt_to_pr = dt_pr[gt_b]\n",
        "\n",
        "    all_d = np.concatenate([d_pr_to_gt, d_gt_to_pr])\n",
        "    assd = float(all_d.mean())\n",
        "    hd95 = float(np.percentile(all_d, 95))\n",
        "    return assd, hd95\n",
        "\n",
        "# threshold selection- tune on mean tolerant-F1 across TOL_PX_LIST (not Dice)\n",
        "def find_best_global_threshold(predict_fn, val_samples, thresholds, limit=None):\n",
        "    best_thr, best_mean = 0.5, -1.0\n",
        "    for t in thresholds:\n",
        "        scores = []\n",
        "        count = 0\n",
        "        for d in val_samples:\n",
        "            out = load_sample(d, target_mode=\"boundary\")\n",
        "            if out is None:\n",
        "                continue\n",
        "            img01, gt = out\n",
        "            pr = predict_fn(img01).numpy().squeeze().astype(np.float32)\n",
        "            gt = gt.numpy().squeeze().astype(np.float32)\n",
        "            pr = np.clip(pr, 0, 1)\n",
        "\n",
        "            pr_bin = (pr >= t).astype(np.uint8)\n",
        "            gt_bin = (gt >= 0.5).astype(np.uint8)\n",
        "\n",
        "            f = float(np.mean([tolerant_f1(gt_bin, pr_bin, tol_px=px) for px in TOL_PX_LIST]))\n",
        "            scores.append(f)\n",
        "\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "\n",
        "        mean_s = float(np.mean(scores)) if len(scores) else 0.0\n",
        "        if mean_s > best_mean:\n",
        "            best_mean = mean_s\n",
        "            best_thr = t\n",
        "    return best_thr, best_mean\n",
        "\n",
        "# UNet threshold selection: also tune on mean tolerant-F1 across TOL_PX_LIST\n",
        "def find_best_threshold_for_unet_boundary(unet_predict_mask_fn, val_samples, thresholds, limit=None):\n",
        "    best_thr, best_mean = 0.5, -1.0\n",
        "    for t in thresholds:\n",
        "        scores = []\n",
        "        count = 0\n",
        "        for d in val_samples:\n",
        "            out_b = load_sample(d, target_mode=\"boundary\")\n",
        "            out_m = load_sample(d, target_mode=\"mask\")\n",
        "            if out_b is None or out_m is None:\n",
        "                continue\n",
        "            img01, gt_b = out_b\n",
        "\n",
        "            pr_m = unet_predict_mask_fn(img01).numpy().squeeze()\n",
        "            pr_m = (pr_m >= t).astype(np.float32)\n",
        "\n",
        "            pr_m_tf = tf.convert_to_tensor(pr_m[..., None], dtype=tf.float32)\n",
        "            pr_b = mask_boundary_target(pr_m_tf).numpy().squeeze().astype(np.float32)\n",
        "\n",
        "            gt_b_np = gt_b.numpy().squeeze().astype(np.float32)\n",
        "            gt_bin = (gt_b_np >= 0.5).astype(np.uint8)\n",
        "            pr_bin = (pr_b >= 0.5).astype(np.uint8)\n",
        "\n",
        "            f = float(np.mean([tolerant_f1(gt_bin, pr_bin, tol_px=px) for px in TOL_PX_LIST]))\n",
        "            scores.append(f)\n",
        "\n",
        "            count += 1\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "\n",
        "        mean_s = float(np.mean(scores)) if len(scores) else 0.0\n",
        "        if mean_s > best_mean:\n",
        "            best_mean = mean_s\n",
        "            best_thr = t\n",
        "    return best_thr, best_mean\n",
        "\n",
        "def eval_boundary_method(name, predict_boundary_fn, test_samples, thr_global, tol_list, limit=None):\n",
        "    rows = []\n",
        "    count = 0\n",
        "    for d in test_samples:\n",
        "        out = load_sample(d, target_mode=\"boundary\")\n",
        "        if out is None:\n",
        "            continue\n",
        "        img01, gt = out\n",
        "        gt_b = gt.numpy().squeeze().astype(np.float32)\n",
        "\n",
        "        pr = predict_boundary_fn(img01).numpy().squeeze().astype(np.float32)\n",
        "        pr = np.clip(pr, 0, 1)\n",
        "\n",
        "        dice05, p05, r05 = bin_metrics(gt_b, pr, thr=0.5)\n",
        "        diceg, pg, rg = bin_metrics(gt_b, pr, thr=thr_global)\n",
        "\n",
        "        pr_bin = (pr >= thr_global).astype(np.uint8)\n",
        "        gt_bin = (gt_b >= 0.5).astype(np.uint8)\n",
        "\n",
        "        assd, hd95 = assd_hd95(gt_bin.astype(bool), pr_bin.astype(bool))\n",
        "\n",
        "        row = {\n",
        "            \"method\": name,\n",
        "            \"Dice@0.5\": dice05,\n",
        "            \"Dice@valThr\": diceg,\n",
        "            \"Prec@valThr\": pg,\n",
        "            \"Rec@valThr\": rg,\n",
        "            \"ASSD_px\": assd,\n",
        "            \"HD95_px\": hd95,\n",
        "            \"Thr_val\": thr_global,\n",
        "        }\n",
        "        for tol in tol_list:\n",
        "            row[f\"F1@tol{tol}px\"] = tolerant_f1(gt_bin, pr_bin, tol_px=tol)\n",
        "\n",
        "        rows.append(row)\n",
        "        count += 1\n",
        "        if limit and count >= limit:\n",
        "            break\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def overlay_edges(img01, gt_b=None, pr_b=None, thr=0.5):\n",
        "    img = img01.numpy().squeeze()\n",
        "    img = (img - img.min())/(img.max()-img.min()+1e-8)\n",
        "    rgb = np.stack([img,img,img], axis=-1)\n",
        "\n",
        "    gt = None\n",
        "    pr = None\n",
        "    if gt_b is not None:\n",
        "        gt = (gt_b.numpy().squeeze() >= 0.5)\n",
        "    if pr_b is not None:\n",
        "        pr = (pr_b >= thr)\n",
        "\n",
        "    if gt is not None:\n",
        "        rgb[gt] = [0,1,0]\n",
        "    if pr is not None:\n",
        "        rgb[pr] = [1,0,0]\n",
        "    if gt is not None and pr is not None:\n",
        "        both = gt & pr\n",
        "        rgb[both] = [1,1,0]\n",
        "    return rgb\n",
        "\n",
        "def show_overlay_panel(models_dict, title, samples, thr_map, k=3, save_path=None):\n",
        "    chosen = []\n",
        "    for d in samples:\n",
        "        out = load_sample(d, target_mode=\"boundary\")\n",
        "        if out is not None:\n",
        "            chosen.append((d, out[0], out[1]))\n",
        "        if len(chosen) >= k:\n",
        "            break\n",
        "    if len(chosen) == 0:\n",
        "        print(\"No labeled samples for overlay.\")\n",
        "        return\n",
        "\n",
        "    cols = 1 + len(models_dict)\n",
        "    plt.figure(figsize=(4*cols, 4*len(chosen)))\n",
        "    for r, (_, img01, gt_b) in enumerate(chosen):\n",
        "        plt.subplot(len(chosen), cols, r*cols + 1)\n",
        "        plt.imshow(overlay_edges(img01, gt_b=gt_b, pr_b=None, thr=0.5))\n",
        "        plt.title(\"GT (green)\"); plt.axis(\"off\")\n",
        "\n",
        "        c = 2\n",
        "        for name, model in models_dict.items():\n",
        "            pr = model(img01[None], training=False)[0].numpy().squeeze()\n",
        "            thr = thr_map.get(name, 0.5)\n",
        "            plt.subplot(len(chosen), cols, r*cols + c)\n",
        "            plt.imshow(overlay_edges(img01, gt_b=gt_b, pr_b=pr, thr=thr))\n",
        "            plt.title(f\"{name} (red), overlap(yellow)\"); plt.axis(\"off\")\n",
        "            c += 1\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "def show_inference_panel(models_dict, title, samples, thr=0.5, k=3, save_path=None):\n",
        "    chosen = []\n",
        "    for d in samples:\n",
        "        img01 = load_input_only(d)\n",
        "        if img01 is not None:\n",
        "            chosen.append(img01)\n",
        "        if len(chosen) >= k:\n",
        "            break\n",
        "    if len(chosen) == 0:\n",
        "        print(\"No inference samples.\")\n",
        "        return\n",
        "\n",
        "    cols = 1 + len(models_dict)\n",
        "    plt.figure(figsize=(4*cols, 4*len(chosen)))\n",
        "    for r, img01 in enumerate(chosen):\n",
        "        plt.subplot(len(chosen), cols, r*cols + 1)\n",
        "        plt.imshow(img01.numpy().squeeze(), cmap=\"gray\"); plt.title(\"Input\"); plt.axis(\"off\")\n",
        "\n",
        "        c = 2\n",
        "        for name, model in models_dict.items():\n",
        "            pr = model(img01[None], training=False)[0].numpy().squeeze()\n",
        "            ov = overlay_edges(img01, gt_b=None, pr_b=pr, thr=thr)\n",
        "            plt.subplot(len(chosen), cols, r*cols + c)\n",
        "            plt.imshow(ov); plt.title(name); plt.axis(\"off\")\n",
        "            c += 1\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "\n",
        "# OUTPUT DIRS\n",
        "BASE_DIR = \"/content/embc_run\"\n",
        "OUT_DIR  = os.path.join(BASE_DIR, \"outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "dataset_roots = {}\n",
        "if USE_GOOGLE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    if not os.path.isfile(BRATS_ZIP_PATH):\n",
        "        raise ValueError(f\"BraTS zip not found at: {BRATS_ZIP_PATH}\")\n",
        "\n",
        "    sentinel = os.path.join(BRATS_EXTRACT_DIR, \".extracted_ok\")\n",
        "    if not os.path.exists(sentinel):\n",
        "        if os.path.exists(BRATS_EXTRACT_DIR):\n",
        "            shutil.rmtree(BRATS_EXTRACT_DIR)\n",
        "        os.makedirs(BRATS_EXTRACT_DIR, exist_ok=True)\n",
        "        print(\"Unzipping BraTS zip (this can take a bit)...\")\n",
        "        with zipfile.ZipFile(BRATS_ZIP_PATH, \"r\") as z:\n",
        "            z.extractall(BRATS_EXTRACT_DIR)\n",
        "        with open(sentinel, \"w\") as f:\n",
        "            f.write(\"ok\")\n",
        "        print(\"Unzipped BraTS to:\", BRATS_EXTRACT_DIR)\n",
        "    else:\n",
        "        print(\" BraTS already unzipped at:\", BRATS_EXTRACT_DIR)\n",
        "\n",
        "    train_hits = (\n",
        "        glob.glob(os.path.join(BRATS_EXTRACT_DIR, \"**/MICCAI_BraTS2020_TrainingData\"), recursive=True) +\n",
        "        glob.glob(os.path.join(BRATS_EXTRACT_DIR, \"**/BraTS2020_TrainingData\"), recursive=True)\n",
        "    )\n",
        "    val_hits = (\n",
        "        glob.glob(os.path.join(BRATS_EXTRACT_DIR, \"**/MICCAI_BraTS2020_ValidationData\"), recursive=True) +\n",
        "        glob.glob(os.path.join(BRATS_EXTRACT_DIR, \"**/BraTS2020_ValidationData\"), recursive=True)\n",
        "    )\n",
        "\n",
        "    def best_dir_with_nii(hits):\n",
        "        hits = [h for h in hits if os.path.isdir(h)]\n",
        "        scored = []\n",
        "        for h in hits:\n",
        "            n = len(glob.glob(os.path.join(h, \"**/*.nii*\"), recursive=True))\n",
        "            scored.append((n, len(h), h))\n",
        "        return max(scored)[2] if scored else None\n",
        "\n",
        "    BRATS_TRAIN_DIR = best_dir_with_nii(train_hits)\n",
        "    BRATS_VAL_DIR   = best_dir_with_nii(val_hits)\n",
        "\n",
        "    if not BRATS_TRAIN_DIR or not BRATS_VAL_DIR:\n",
        "        raise ValueError(\"Could not auto-detect BraTS Training/Validation folders inside the zip.\")\n",
        "\n",
        "    dataset_roots[\"BraTS2020_train\"] = BRATS_TRAIN_DIR\n",
        "    dataset_roots[\"BraTS2020_official_val_unlabeled\"] = BRATS_VAL_DIR\n",
        "\n",
        "    for k,v in EXTRA_DATASET_DIRS.items():\n",
        "        dataset_roots[k] = v\n",
        "\n",
        "print(\"\\nDataset roots:\", dataset_roots)\n",
        "\n",
        "# Build indices\n",
        "train_samples_all, has_masks_train = build_index_for_dataset(dataset_roots[\"BraTS2020_train\"])\n",
        "official_val_samples, has_masks_off = build_index_for_dataset(dataset_roots[\"BraTS2020_official_val_unlabeled\"])\n",
        "\n",
        "if not has_masks_train:\n",
        "    raise ValueError(\"BraTS TrainingData masks not detected. Check tokens / folder structure.\")\n",
        "\n",
        "print(\"\\nBraTS index sizes:\")\n",
        "print(\"  TrainingData slices:\", len(train_samples_all), \" labeled=\", has_masks_train)\n",
        "print(\"  Official Val slices:\", len(official_val_samples), \" labeled=\", has_masks_off)\n",
        "\n",
        "# MAIN: TRAIN/EVAL per seed\n",
        "\n",
        "all_seed_masters = []\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"SEED = {seed}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    set_all_seeds(seed)\n",
        "\n",
        "    # Leak-free patient-level split\n",
        "    train_s, val_s, test_labeled_s = split_by_group(train_samples_all, TRAIN_FRAC, VAL_FRAC, seed=seed)\n",
        "    print(f\"\\nLeak-free patient split:\")\n",
        "    print(\"  train:\", len(train_s))\n",
        "    print(\"  val  :\", len(val_s))\n",
        "    print(\"  test_labeled:\", len(test_labeled_s))\n",
        "\n",
        "    # Compute pos_weight from training boundary targets\n",
        "    pos_w = estimate_pos_weight(train_s, limit=300)\n",
        "    b_loss = boundary_loss_factory(pos_w)\n",
        "    print(\"Boundary pos_weight:\", pos_w)\n",
        "\n",
        "    # Datasets\n",
        "    pretrain_ds = make_tf_dataset(train_s, target_mode=\"sobel\", shuffle=True, repeat=True, seed=seed)\n",
        "    pre_val_ds  = make_tf_dataset(val_s,   target_mode=\"sobel\", shuffle=False, repeat=True, seed=seed)\n",
        "\n",
        "    finetune_ds = make_tf_dataset(train_s, target_mode=\"boundary\", shuffle=True, repeat=True, seed=seed)\n",
        "    fin_val_ds  = make_tf_dataset(val_s,   target_mode=\"boundary\", shuffle=False, repeat=True, seed=seed)\n",
        "\n",
        "    unet_ds     = make_tf_dataset(train_s, target_mode=\"mask\", shuffle=True, repeat=True, seed=seed)\n",
        "    unet_val_ds = make_tf_dataset(val_s,   target_mode=\"mask\", shuffle=False, repeat=True, seed=seed)\n",
        "\n",
        "    steps_pre  = estimate_steps(train_s)\n",
        "    vsteps_pre = estimate_steps(val_s)\n",
        "    steps_fin  = estimate_steps(train_s)\n",
        "    vsteps_fin = estimate_steps(val_s)\n",
        "\n",
        "    # Models\n",
        "    learnable = build_learnable_model(loss_obj=sobel_mse_loss)\n",
        "    learnable.use_per_image_norm = True\n",
        "    learnable.use_sigmoid = False\n",
        "\n",
        "    if PRETRAIN_SOBEL_EPOCHS > 0:\n",
        "        _ = learnable.fit(\n",
        "            pretrain_ds, validation_data=pre_val_ds,\n",
        "            epochs=PRETRAIN_SOBEL_EPOCHS,\n",
        "            steps_per_epoch=steps_pre, validation_steps=vsteps_pre,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    # Boundary finetune (weighted loss)\n",
        "    learnable.compile(optimizer=tf.keras.optimizers.Adam(LR), loss=b_loss)\n",
        "    learnable.use_per_image_norm = False\n",
        "    learnable.use_sigmoid = True\n",
        "\n",
        "    _ = learnable.fit(\n",
        "        finetune_ds, validation_data=fin_val_ds,\n",
        "        epochs=FINETUNE_BOUNDARY_EPOCHS,\n",
        "        steps_per_epoch=steps_fin, validation_steps=vsteps_fin,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    smallcnn = None\n",
        "    sobelfront = None\n",
        "    unet = None\n",
        "\n",
        "    if RUN_SMALL_CNN_BASELINE:\n",
        "        smallcnn = build_small_cnn_boundary(loss_obj=b_loss)\n",
        "        _ = smallcnn.fit(\n",
        "            finetune_ds, validation_data=fin_val_ds,\n",
        "            epochs=FINETUNE_BOUNDARY_EPOCHS,\n",
        "            steps_per_epoch=steps_fin, validation_steps=vsteps_fin,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    if RUN_SOBELFRONT_CNN:\n",
        "        sobelfront = build_sobelfront_cnn(loss_obj=b_loss)\n",
        "        _ = sobelfront.fit(\n",
        "            finetune_ds, validation_data=fin_val_ds,\n",
        "            epochs=FINETUNE_BOUNDARY_EPOCHS,\n",
        "            steps_per_epoch=steps_fin, validation_steps=vsteps_fin,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    if RUN_TINY_UNET_SEG_BASE:\n",
        "        unet = build_tiny_unet_seg(loss_obj=seg_loss)\n",
        "        _ = unet.fit(\n",
        "            unet_ds, validation_data=unet_val_ds,\n",
        "            epochs=FINETUNE_UNET_EPOCHS,\n",
        "            steps_per_epoch=steps_fin, validation_steps=vsteps_fin,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    # Predict fns (boundary)\n",
        "    def pred_fixed(img01): return fixed_sobel_conv(img01)\n",
        "    def pred_learnable(img01): return learnable(img01[None], training=False)[0]\n",
        "    def pred_smallcnn(img01): return smallcnn(img01[None], training=False)[0]\n",
        "    def pred_sobelfront(img01): return sobelfront(img01[None], training=False)[0]\n",
        "    def pred_unet_mask(img01): return unet(img01[None], training=False)[0]\n",
        "\n",
        "    # Threshold selection on VAL (obj = mean tolerant F1)\n",
        "    thr_fixed, best_fixed = find_best_global_threshold(pred_fixed, val_s, THRESHOLDS, limit=EVAL_LIMIT)\n",
        "    thr_learn, best_learn = find_best_global_threshold(pred_learnable, val_s, THRESHOLDS, limit=EVAL_LIMIT)\n",
        "\n",
        "    thr_small = None\n",
        "    thr_sf = None\n",
        "    thr_unet = None\n",
        "\n",
        "    if smallcnn:\n",
        "        thr_small, best_small = find_best_global_threshold(pred_smallcnn, val_s, THRESHOLDS, limit=EVAL_LIMIT)\n",
        "    if sobelfront:\n",
        "        thr_sf, best_sf = find_best_global_threshold(pred_sobelfront, val_s, THRESHOLDS, limit=EVAL_LIMIT)\n",
        "    if unet:\n",
        "        thr_unet, best_unet = find_best_threshold_for_unet_boundary(pred_unet_mask, val_s, THRESHOLDS, limit=EVAL_LIMIT)\n",
        "\n",
        "    print(\"\\nVAL-tuned global thresholds (tuned on mean tolerant-F1):\")\n",
        "    print(\"  FixedSobel :\", thr_fixed, \"score:\", best_fixed)\n",
        "    print(\"  Learnable  :\", thr_learn, \"score:\", best_learn)\n",
        "    if thr_small is not None: print(\"  SmallCNN   :\", thr_small)\n",
        "    if thr_sf is not None:    print(\"  SobelFront :\", thr_sf)\n",
        "    if thr_unet is not None:  print(\"  TinyUNet(mask->boundary) mask_thr:\", thr_unet)\n",
        "\n",
        "    # Evaluate on TEST_LABELED (no leakage)\n",
        "    print(\"\\n PAPER EVAL: TEST_LABELED (leak-free, patient split)\\n\")\n",
        "\n",
        "    # boundary from unet at chosen mask threshold\n",
        "    def pred_unet_boundary(img01):\n",
        "        pr_m = pred_unet_mask(img01).numpy().squeeze()\n",
        "        pr_m = (pr_m >= thr_unet).astype(np.float32)\n",
        "        pr_m_tf = tf.convert_to_tensor(pr_m[..., None], dtype=tf.float32)\n",
        "        pr_b = mask_boundary_target(pr_m_tf).numpy().squeeze().astype(np.float32)\n",
        "        return tf.convert_to_tensor(pr_b[..., None], dtype=tf.float32)\n",
        "\n",
        "    # Per-method eval\n",
        "    df_fixed = eval_boundary_method(\"B_fixed_sobel_edges\", pred_fixed, test_labeled_s, thr_fixed, TOL_PX_LIST, limit=EVAL_LIMIT)\n",
        "    df_learn = eval_boundary_method(\"C_learnable_sobel_pretrain_finetune\", pred_learnable, test_labeled_s, thr_learn, TOL_PX_LIST, limit=EVAL_LIMIT)\n",
        "\n",
        "    dfs = [df_fixed, df_learn]\n",
        "\n",
        "    if smallcnn:\n",
        "        df_small = eval_boundary_method(\"D_smallcnn_baseline\", pred_smallcnn, test_labeled_s, thr_small, TOL_PX_LIST, limit=EVAL_LIMIT)\n",
        "        dfs.append(df_small)\n",
        "    if sobelfront:\n",
        "        df_sf = eval_boundary_method(\"E_sobelfront_cnn\", pred_sobelfront, test_labeled_s, thr_sf, TOL_PX_LIST, limit=EVAL_LIMIT)\n",
        "        dfs.append(df_sf)\n",
        "    if unet:\n",
        "        df_unet = eval_boundary_method(\"F_tinyunet_seg_to_boundary\", lambda img01: pred_unet_boundary(img01), test_labeled_s, 0.5, TOL_PX_LIST, limit=EVAL_LIMIT)\n",
        "        df_unet[\"Thr_val\"] = thr_unet\n",
        "        dfs.append(df_unet)\n",
        "\n",
        "    df_all = pd.concat(dfs, ignore_index=True)\n",
        "    df_all[\"seed\"] = seed\n",
        "\n",
        "    # Summarize (mean/std) per seed\n",
        "    metric_cols = [\"Dice@0.5\",\"Dice@valThr\",\"Prec@valThr\",\"Rec@valThr\",\"ASSD_px\",\"HD95_px\"] + [f\"F1@tol{t}px\" for t in TOL_PX_LIST]\n",
        "    summary = (df_all.groupby(\"method\")[metric_cols]\n",
        "               .agg([\"mean\",\"std\"])\n",
        "               .reset_index())\n",
        "    summary.columns = [\"_\".join(c).strip(\"_\") for c in summary.columns]\n",
        "    summary[\"seed\"] = seed\n",
        "    summary[\"alpha\"] = float(learnable.alpha.numpy())\n",
        "    summary[\"beta\"]  = float(learnable.beta.numpy())\n",
        "    summary[\"pos_weight\"] = float(pos_w)\n",
        "    summary[\"learnable_params\"] = learnable.count_params()\n",
        "    summary[\"smallcnn_params\"]  = smallcnn.count_params() if smallcnn else None\n",
        "    summary[\"sobelfront_params\"]= sobelfront.count_params() if sobelfront else None\n",
        "    summary[\"unet_params\"]      = unet.count_params() if unet else None\n",
        "\n",
        "    # Perfect row (for reference)\n",
        "    perfect_row = {\"method_\": \"A_perfect_target_vs_target\", \"seed\": seed,\n",
        "                   \"alpha\": float(learnable.alpha.numpy()),\n",
        "                   \"beta\": float(learnable.beta.numpy()),\n",
        "                   \"pos_weight\": float(pos_w),\n",
        "                   \"learnable_params\": learnable.count_params(),\n",
        "                   \"smallcnn_params\": smallcnn.count_params() if smallcnn else None,\n",
        "                   \"sobelfront_params\": sobelfront.count_params() if sobelfront else None,\n",
        "                   \"unet_params\": unet.count_params() if unet else None}\n",
        "\n",
        "    # Fill perfect metrics\n",
        "    for col in metric_cols:\n",
        "        if col in [\"ASSD_px\",\"HD95_px\"]:\n",
        "            perfect_row[f\"{col}_mean\"] = 0.0\n",
        "            perfect_row[f\"{col}_std\"]  = 0.0\n",
        "        else:\n",
        "            perfect_row[f\"{col}_mean\"] = 1.0\n",
        "            perfect_row[f\"{col}_std\"]  = 0.0\n",
        "\n",
        "    perfect_row[\"Thr_val_mean\"] = 0.5\n",
        "    perfect_row[\"Thr_val_std\"]  = 0.0\n",
        "\n",
        "    # Save per-seed outputs\n",
        "    seed_dir = os.path.join(OUT_DIR, f\"seed_{seed}\")\n",
        "    os.makedirs(seed_dir, exist_ok=True)\n",
        "\n",
        "    df_all.to_csv(os.path.join(seed_dir, \"per_sample_metrics.csv\"), index=False)\n",
        "    summary.to_csv(os.path.join(seed_dir, \"summary_metrics.csv\"), index=False)\n",
        "\n",
        "    # Overlays on TEST_LABELED\n",
        "    thr_map = {\n",
        "        \"Learnable\": thr_learn,\n",
        "        \"SmallCNN\": thr_small if thr_small is not None else 0.5,\n",
        "        \"SobelFrontCNN\": thr_sf if thr_sf is not None else 0.5,\n",
        "    }\n",
        "\n",
        "    overlay_models = {\"Learnable\": learnable}\n",
        "    if smallcnn: overlay_models[\"SmallCNN\"] = smallcnn\n",
        "    if sobelfront: overlay_models[\"SobelFrontCNN\"] = sobelfront\n",
        "\n",
        "    show_overlay_panel(\n",
        "        overlay_models,\n",
        "        title=f\"BraTS TEST_LABELED (seed {seed}): GT green, Pred red, overlap yellow\",\n",
        "        samples=test_labeled_s,\n",
        "        thr_map=thr_map,\n",
        "        k=VIS_K,\n",
        "        save_path=os.path.join(seed_dir, \"FIG_overlay_TEST_LABELED.png\")\n",
        "    )\n",
        "\n",
        "    # Inference-only on official unlabeled val\n",
        "    inf_models = {\"Learnable(boundary)\": learnable}\n",
        "    if smallcnn: inf_models[\"SmallCNN(boundary)\"] = smallcnn\n",
        "    if sobelfront: inf_models[\"SobelFront(boundary)\"] = sobelfront\n",
        "\n",
        "    show_inference_panel(\n",
        "        inf_models,\n",
        "        title=f\"BraTS official ValidationData (unlabeled) boundary inference (seed {seed})\",\n",
        "        samples=official_val_samples,\n",
        "        thr=thr_map.get(\"SobelFrontCNN\", 0.5),\n",
        "        k=VIS_K,\n",
        "        save_path=os.path.join(seed_dir, \"FIG_infer_OFFICIAL_VAL.png\")\n",
        "    )\n",
        "\n",
        "    # Build a \"master rows\" table for this seed: perfect + method summaries\n",
        "    master_rows = [perfect_row]\n",
        "    for _, row in summary.iterrows():\n",
        "        d = row.to_dict()\n",
        "        # unify method name column to method_\n",
        "        d[\"method_\"] = d.get(\"method_\", None)\n",
        "        if \"method_\" not in d or d[\"method_\"] is None:\n",
        "            # summary has \"method\" column name turned into \"method_\"? It is \"method_\" after groupby flatten\n",
        "            # safest: recover from row keys\n",
        "            if \"method_\" in row:\n",
        "                d[\"method_\"] = row[\"method_\"]\n",
        "            elif \"method\" in row:\n",
        "                d[\"method_\"] = row[\"method\"]\n",
        "        master_rows.append(d)\n",
        "\n",
        "    all_seed_masters.append(pd.DataFrame(master_rows))\n",
        "\n",
        "# MASTER EXPORT (all seeds)\n",
        "\n",
        "master = pd.concat(all_seed_masters, ignore_index=True) if len(all_seed_masters) else pd.DataFrame()\n",
        "master_path = os.path.join(OUT_DIR, \"embc_paper_master_summary_all_seeds.csv\")\n",
        "master.to_csv(master_path, index=False)\n",
        "\n",
        "print(\"\\n Saved master summary:\", master_path)\n",
        "print(\"\\n===== MASTER SUMMARY (preview) =====\")\n",
        "print(master.head(20) if len(master) else \"Empty master table\")\n",
        "\n",
        "# Across-seed aggregate (mean/std across SEEDS) for each method_\n",
        "m = master[master[\"method_\"].notna()].copy()\n",
        "num_cols = [c for c in m.columns if c not in [\"method_\", \"method\", \"seed\"] and pd.api.types.is_numeric_dtype(m[c])]\n",
        "agg = (m.groupby(\"method_\")[num_cols].agg([\"mean\",\"std\"]).reset_index())\n",
        "agg.columns = [\"_\".join(col).strip(\"_\") for col in agg.columns]\n",
        "agg_path = os.path.join(OUT_DIR, \"embc_summary_across_seeds.csv\")\n",
        "agg.to_csv(agg_path, index=False)\n",
        "print(\" Saved across-seed summary:\", agg_path)\n",
        "\n",
        "# Save reproducibility config\n",
        "config = dict(\n",
        "    IMG_SIZE=IMG_SIZE, BATCH_SIZE=BATCH_SIZE, LR=LR,\n",
        "    SEEDS=SEEDS,\n",
        "    PRETRAIN_SOBEL_EPOCHS=PRETRAIN_SOBEL_EPOCHS,\n",
        "    FINETUNE_BOUNDARY_EPOCHS=FINETUNE_BOUNDARY_EPOCHS,\n",
        "    FINETUNE_UNET_EPOCHS=FINETUNE_UNET_EPOCHS,\n",
        "    MAX_SLICES_PER_VOLUME=MAX_SLICES_PER_VOLUME,\n",
        "    TRAIN_FRAC=TRAIN_FRAC, VAL_FRAC=VAL_FRAC,\n",
        "    THRESHOLDS=THRESHOLDS,\n",
        "    TOL_PX_LIST=TOL_PX_LIST,\n",
        "    SKIP_EMPTY_MASK_SLICES=SKIP_EMPTY_MASK_SLICES,\n",
        "    BOUNDARY_THICKNESS=BOUNDARY_THICKNESS,\n",
        "    RUN_SMALL_CNN_BASELINE=RUN_SMALL_CNN_BASELINE,\n",
        "    RUN_SOBELFRONT_CNN=RUN_SOBELFRONT_CNN,\n",
        "    RUN_TINY_UNET_SEG_BASE=RUN_TINY_UNET_SEG_BASE,\n",
        "    BRATS_ZIP_PATH=BRATS_ZIP_PATH\n",
        ")\n",
        "with open(os.path.join(OUT_DIR, \"run_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# Zip outputs\n",
        "zip_out = \"/content/embc_edge_outputs.zip\"\n",
        "!zip -qr {zip_out} {OUT_DIR}\n",
        "files.download(zip_out)\n",
        "\n",
        "print(\"\\n DONE. Outputs include:\")\n",
        "print(\" - per-seed per-sample + summary CSVs\")\n",
        "print(\" - master summary across seeds: embc_paper_master_summary_all_seeds.csv\")\n",
        "print(\" - across-seed summary: embc_summary_across_seeds.csv\")\n",
        "print(\" - overlays on TEST_LABELED + inference on official unlabeled val\")\n",
        "print(\" - run_config.json for reproducibility\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73qvDM61uJhl",
        "outputId": "f95e5b8f-2ed5-4149-f5e7-909bb46bd7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "IMG_SIZE: (256, 256)\n",
            "Mounted at /content/drive\n",
            "Unzipping BraTS zip (this can take a bit)...\n"
          ]
        }
      ]
    }
  ]
}
